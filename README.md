<!-- <div align="center"> -->

# üé• ‚ûï üìù ‚û°Ô∏è üé• Composed Video Retrieval via Enriched Context and Discriminative Embeddings (CVPR-2024)

<p align="center">
   <a href="https://github.com/mbzuai-oryx/MobiLlama/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="license"></a>
</p>


#### [Omkar Thawakar](https://scholar.google.com/citations?user=flvl5YQAAAAJ&hl=en), [Muzammal Naseer](https://scholar.google.ch/citations?user=tM9xKA8AAAAJ&hl=en), [Rao Muhammad Anwer](https://scholar.google.com/citations?hl=en&authuser=1&user=_KlvMVoAAAAJ), [Salman Khan](https://salman-h-khan.github.io/), [Michael Felsberg](https://scholar.google.com/citations?user=lkWfR08AAAAJ&hl=en), [Mubarak Shah](https://scholar.google.ch/citations?user=p8gsO3gAAAAJ&hl=en) and [Fahad Khan](https://sites.google.com/view/fahadkhans/home)

#### **Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), UAE**


[![GitHub Stars](https://img.shields.io/github/stars/OmkarThawakar/composed-video-retrieval?style=social)](https://github.com/OmkarThawakar/composed-video-retrieval)


<p align="center">
    <img src="https://i.imgur.com/waxVImv.png" alt="hline">
</p>

<!-- </div> -->
## Overview
<div align="justify">

> Composed video retrieval (CoVR) is a challenging problem in computer vision which has recently highlighted the integration of modification text with visual queries for more sophisticated video search in large databases. Existing works
predominantly rely on visual queries combined with modification text to distinguish relevant videos. However, such
a strategy struggles to fully preserve the rich query-specific
context in retrieved target videos and only represents the
target video using visual embedding. We introduce a novel
CoVR framework that leverages detailed language descriptions to explicitly encode query-specific contextual information and learns discriminative embeddings of vision only,
text only and vision-text for better alignment to accurately
retrieve matched target videos. Our proposed framework can
be flexibly employed for both composed video (CoVR) and
image (CoIR) retrieval tasks. Experiments on three datasets
show that our approach obtains state-of-the-art performance
for both CovR and zero-shot CoIR tasks, achieving gains
as high as around 7% in terms of recall@K=1 score.

</div>

## Dataset
To download the webvid-covr videos, install [`mpi4py`](https://mpi4py.readthedocs.io/en/latest/install.html#) and run:
```bash
python tools/scripts/download_covr.py <split>
```
To download the annotations of webvid-covr: 
```bash
bash tools/scripts/download_annotation.sh covr
```

## Generate Descriptions (optional)
To generate the descriptions of webvid-covr videos, use script `tools/scripts/generate_webvid_description_2m.py` and `tools/scripts/generate_webvid_description_8m.py` inside main directory of [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)


## Download webvid-covr annotations with our generated descriptions 
Download the webvid-covr annotation files with our generated descriptions from here : [OneDrive Link](https://mbzuaiac-my.sharepoint.com/:f:/g/personal/omkar_thawakar_mbzuai_ac_ae/EmOQrLWr6oxCmH1k7PNKUoABhV8XoOsQZjVqdkJwp9jYiw?e=yOnA93)


## Model Checkpoints 
Download the model checkpoints from here : [OneDrive Link](https://mbzuaiac-my.sharepoint.com/:f:/g/personal/omkar_thawakar_mbzuai_ac_ae/EsyegY3ZGj9KucaI9u_stFkBGul2A_aEi89mZBKbkFQpmA?e=oaIV1D). 
Save the checkpoint in folder structure : `outputs/webvid-covr/blip-large/blip-l-coco/tv-False_loss-hnnce_lr-1e-05/`

Final repository contains: 

```markdown
üì¶ composed-video-retrieval
 ‚î£ üìÇ annotations
 ‚î£ üìÇ configs 
 ‚î£ üìÇ datasets 
 ‚î£ üìÇ outputs                
 ‚î£ üìÇ src                     
 ‚î£ üìÇ tools                   
 ‚î£ üìú LICENSE
 ‚î£ üìú README.md
 ‚î£ üìú test.py
 ‚îó üìú train.py

 ```

## Installation 

<summary>Create environment</summary> 

```bash
conda create --name covr
conda activate covr
```

Install the following packages inside the conda environment:

```bash
pip install -r requirements.txt
```

The code was tested on Python 3.10 and PyTorch >= 2.0.


## Usage :computer:
<summary>Computing BLIP embeddings</summary>
&emsp; 

Before training, you will need to compute the BLIP embeddings for the videos/images. To do so, run:
```bash
python tools/embs/save_blip_embs_vids.py # This will compute the embeddings for the WebVid-CoVR videos.
python tools/embs/save_blip_embs_imgs.py # This will compute the embeddings for the CIRR or FashionIQ images.
```


### Training

The command to launch a training experiment is the folowing:
```bash
python train.py [OPTIONS]
```
The parsing is done by using the powerful [Hydra](https://github.com/facebookresearch/hydra) library. You can override anything in the configuration by passing arguments like ``foo=value`` or ``foo.bar=value``.


### Evaluation

The command to evaluate is the folowing:
```bash
python test.py test=<test> [OPTIONS]
``` 


### Options parameters

#### Datasets:
- ``data=webvid-covr``: WebVid-CoVR datasets.
- ``data=cirr``: CIRR dataset.
- ``data=fashioniq-split``: FashionIQ dataset, change ``split`` to ``dress``, ``shirt`` or ``toptee``.

#### Tests:
- ``test=all``: Test on WebVid-CoVR, CIRR and all three Fashion-IQ test sets.
- ``test=webvid-covr``: Test on WebVid-CoVR.
- ``test=cirr``: Test on CIRR.
- ``test=fashioniq``: Test on all three Fashion-IQ test sets (``dress``, ``shirt`` and ``toptee``).

#### Checkpoints:
- ``model/ckpt=blip-l-coco``: Default checkpoint for BLIP-L finetuned on COCO.
- ``model/ckpt=webvid-covr``: Default checkpoint for CoVR finetuned on WebVid-CoVR.

#### Training
- ``trainer=gpu``: training with CUDA, change ``devices`` to the number of GPUs you want to use.
- ``trainer=ddp``: training with Distributed Data Parallel (DDP), change ``devices`` and ``num_nodes`` to the number of GPUs and number of nodes you want to use.
- ``trainer=cpu``: training on the CPU (not recommended).

#### Logging
- ``trainer/logger=csv``: log the results in a csv file. Very basic functionality.
- ``trainer/logger=wandb``: log the results in [wandb](https://wandb.ai/). This requires to install ``wandb`` and to set up your wandb account. This is what we used to log our experiments.
- ``trainer/logger=<other>``: Other loggers (not tested).

#### Machine
- ``machine=server``: You can change the default path to the dataset folder and the batch size. You can create your own machine configuration by adding a new file in ``configs/machine``.

#### Experiment
There are many pre-defined experiments from the paper in ``configs/experiments``. Simply add ``experiment=<experiment>`` to the command line to use them. 

### SLURM setting
Use `slurm_train.sh` and `slurm_test.sh` in case of slurm setting. 


## Acknowledgements
- We built our approach using [CoVR-BLIP](https://github.com/lucas-ventura/CoVR) and [BLIP](https://github.com/salesforce/BLIP/) using [lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template/tree/main) in the backend.
- To generate Video descriptions we used [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4). 


## Citation

```bibtex

```